import pandas as pd
import numpy as np 
import matplotlib.pyplot as plt
from sklearn.datasets import fetch_california_housing

housing = fetch_california_housing(as_frame=True)
df = housing.frame
print(df)

def gradient_descent(m_now, b_now, points, alpha):
    x = points['MedInc'].values
    y = points['MedHouseVal'].values

    n = len(x)
    
    y_pred = m_now * x + b_now
    m_gradient = -(2/n) * sum(x * (y - y_pred))
    b_gradient = -(2/n) * sum(y - y_pred)
    
    m = m_now - alpha * m_gradient
    b = b_now - alpha * b_gradient

    return m, b
m = 0
b = 0
alpha = 0.01  
epochs = 1000


for i in range(epochs):
    m, b = gradient_descent(m, b, df, alpha)
    if i % 100 == 0:
        print(f"Epoch {i}: m={m:.4f}, b={b:.4f}")

print("\nFinal values:")
print(f"m = {m}")
print(f"b = {b}")


plt.scatter(df['MedInc'], df['MedHouseVal'], color='black', s=10, label='Data')
x_line = df['MedInc']
y_line = m * x_line + b
plt.plot(x_line, y_line, color='red', label='Best Fit Line')
plt.xlabel("Median Income (MedInc)")
plt.ylabel("Median House Value (MedHouseVal)")
plt.legend()
plt.show()


def compute_mse(m, b, points):
    x = points['MedInc'].values
    y = points['MedHouseVal'].values
    return np.mean((y - (m*x + b))**2)


losses = []

for i in range(epochs):
    m, b = gradient_descent(m, b, df, alpha)
    loss = compute_mse(m, b, df)
    losses.append(loss)

plt.plot(range(epochs), losses)
plt.xlabel("Epochs")
plt.ylabel("MSE Loss")
plt.title("Loss vs Epochs")
plt.show()
